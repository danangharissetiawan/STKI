{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "#All documents\n",
    "doc_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "           28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infix to postifix query\n",
    "def postfix(infix_tokens):\n",
    "    \n",
    "    #precendence initialization\n",
    "    precedence = {}\n",
    "    precedence['NOT'] = 3\n",
    "    precedence['AND'] = 2\n",
    "    precedence['OR'] = 1\n",
    "    precedence['('] = 0\n",
    "    precedence[')'] = 0    \n",
    "\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "    \n",
    "    #creating postfix expression\n",
    "    for token in infix_tokens:\n",
    "        if (token == '('):\n",
    "            operator_stack.append(token)\n",
    "\n",
    "        elif (token == ')'):\n",
    "            operator = operator_stack.pop()\n",
    "            while operator != '(':\n",
    "                output.append(operator)\n",
    "                operator = operator_stack.pop()\n",
    "        \n",
    "        elif (token in precedence):\n",
    "            if (operator_stack):\n",
    "                current_operator = operator_stack[-1]\n",
    "                while (operator_stack and precedence[current_operator] > precedence[token]):\n",
    "                    output.append(operator_stack.pop())\n",
    "                    if (operator_stack):\n",
    "                        current_operator = operator_stack[-1]\n",
    "\n",
    "            operator_stack.append(token)\n",
    "\n",
    "        else:\n",
    "            output.append(token.lower())\n",
    "    \n",
    "    #while staack is not empty appending\n",
    "    while (operator_stack):\n",
    "        output.append(operator_stack.pop())\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND two posting lists\n",
    "def AND_op(word1,word2):\n",
    "    if ((word1) and (word2)):\n",
    "        return set(word1).intersection(word2)\n",
    "    else:\n",
    "        return set()\n",
    "     \n",
    "#OR two posting lists\n",
    "def OR_op(word1, word2):\n",
    "    if word1 is not None and word2 is not None:\n",
    "        return set(word1).union(word2)\n",
    "    else:\n",
    "        return set()\n",
    "   \n",
    "#NOT two posting lists\n",
    "def NOT_op(a,doc_ids):\n",
    "    return set(doc_ids).symmetric_difference(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolean query processing\n",
    "def process_query(q,dictionary_inverted):\n",
    "\n",
    "    q = q.replace('(', '( ')\n",
    "    q = q.replace(')', ' )')\n",
    "    q = q.split(' ')\n",
    "    query = []\n",
    "\n",
    "    for i in q:\n",
    "        query.append(ps.stem(i))\n",
    "    for i in range(0,len(query)):\n",
    "        if ( query[i]== 'and' or query[i]== 'or' or query[i]== 'not'):\n",
    "            query[i] = query[i].upper()\n",
    "    results_stack = []\n",
    "    postfix_queue = postfix(query)\n",
    "\n",
    "    #evaluating postfix query expression\n",
    "    for i in postfix_queue:\n",
    "        if ( i!= 'AND' and i!= 'OR' and i!= 'NOT'):\n",
    "            i = i.replace('(', ' ')\n",
    "            i = i.replace(')', ' ')\n",
    "            i = i.lower()\n",
    "            i = dictionary_inverted.get(i)\n",
    "            results_stack.append(i)\n",
    "        elif (i=='AND'):\n",
    "            a = results_stack.pop()\n",
    "            b = results_stack.pop()\n",
    "            results_stack.append(AND_op(a,b))\n",
    "        elif (i=='OR'):\n",
    "            a = results_stack.pop()\n",
    "            b = results_stack.pop()\n",
    "            results_stack.append(OR_op(a,b))\n",
    "        elif (i == 'NOT'):\n",
    "            a = results_stack.pop()\n",
    "            print(a)\n",
    "            results_stack.append(NOT_op(a,doc_ids))\n",
    "            \n",
    "    return results_stack.pop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating proximity query\n",
    "def positional_query(q,dictionary_positional):\n",
    "    \n",
    "    q = re.sub(r\"AND\", \"\", q)\n",
    "    q = re.sub(r\"  \", \" \", q)\n",
    "    q = q.split(' ')\n",
    "    query = []\n",
    "    \n",
    "    for i in q:\n",
    "        query.append(ps.stem(i))\n",
    "        \n",
    "    word1 = dictionary_positional.get(query[0])\n",
    "    word2 = dictionary_positional.get(query[1])\n",
    "    anding = set(word1).intersection(word2)\n",
    "    \n",
    "    query[2] = re.sub(r\"/\", \"\", query[2])\n",
    "    answer = []\n",
    "    skip = int(query[2]) + 1\n",
    "    for i in anding:\n",
    "        pp1 = dictionary_positional.get(query[0])[i]\n",
    "        pp2 = dictionary_positional.get(query[1])[i]\n",
    "        plen1 = len(pp1)\n",
    "        plen2 = len(pp2)\n",
    "        ii = jj = 0 \n",
    "        while ii != plen1:\n",
    "            while jj != plen2:\n",
    "                if (abs(pp1[ii] - pp2[jj]) == skip):\n",
    "                    answer.append(i)\n",
    "                elif pp2[jj] > pp1[ii]:\n",
    "                    break    \n",
    "                jj+=1\n",
    "            ii+=1\n",
    "    answer = list(dict.fromkeys(answer))\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether word is present within position\n",
    "def doc_check(ii,jj,plen1,plen2,skip,pp1,pp2):\n",
    "    while ii != plen1:\n",
    "        while jj != plen2:\n",
    "            if (abs(pp1[ii] - pp2[jj]) == skip):\n",
    "                return 1\n",
    "            elif pp2[jj] > pp1[ii]:\n",
    "                break\n",
    "            jj+=1\n",
    "        ii+=1\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating phrase query     \n",
    "def phrase_query(q,dictionary_positional,dictionary_inverted):\n",
    "    \n",
    "    q = q.replace('\"', '')\n",
    "    q = q.split()\n",
    "\n",
    "    query = []\n",
    "    for i in q:\n",
    "        query.append(ps.stem(i))\n",
    "        query.append('AND')\n",
    "    query.pop()\n",
    "    query = \" \".join(query)\n",
    "    anding = process_query(query,dictionary_positional)\n",
    "    print(anding)\n",
    "    answer = []\n",
    "    query = query.replace('AND','')\n",
    "    query = query.split()\n",
    "    print(query)\n",
    "\n",
    "    for i in anding:\n",
    "        pp1 = dictionary_positional.get(query[0].lower())[i]\n",
    "        flag = []\n",
    "        skip = 1\n",
    "        for x in range(1,len(query)):\n",
    "            pp2 = dictionary_positional.get(query[x].lower())[i]\n",
    "            plen1 = len(pp1)\n",
    "            plen2 = len(pp2)\n",
    "            ii = jj = 0 \n",
    "            flag.append(doc_check(ii,jj,plen1,plen2,skip,pp1,pp2))\n",
    "            skip = skip + 1\n",
    "        if(0 not in flag):\n",
    "            answer.append(i)\n",
    "    answer = list(dict.fromkeys(answer))\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creates inverted index\n",
    "def inverted_index(stop_words):\n",
    "    \n",
    "    dictionary = {}\n",
    "    documents = {}\n",
    "    \n",
    "    for i in range(0,56):\n",
    "        doc_no = i\n",
    "        with open (\"data/trump_speechs/speech_\" + str(doc_no) + \".txt\",'r') as file:\n",
    "            next(file)\n",
    "            s=file.read().replace('\\n',' ')\n",
    "        \n",
    "        #cleaning documents\n",
    "        s = re.sub('  ', ' ', s)\n",
    "        s = re.sub(r\"won't\", \"will not\", s)\n",
    "        s = re.sub(r\"can\\'t\", \"can not\", s)\n",
    "        s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "        s = re.sub(r\"\\'re\", \" are\", s)\n",
    "        s = re.sub(r\"\\'s\", \" is\", s)\n",
    "        s = re.sub(r\"\\'d\", \" would\", s)\n",
    "        s = re.sub(r\"\\'ll\", \" will\", s)\n",
    "        s = re.sub(r\"\\'t\", \" not\", s)\n",
    "        s = re.sub(r\"\\'ve\", \" have\", s)\n",
    "        s = re.sub(r\"\\'m\", \" am\", s)\n",
    "        s = re.sub(r'[0-9]+', '', s)\n",
    "        s=re.sub(r'[^\\w\\s]',' ', s)\n",
    "        key = 'speech_' + str(doc_no)\n",
    "        \n",
    "        documents.setdefault(key,[])\n",
    "        documents[key].append(s)\n",
    "        \n",
    "        #removing stopwords and lowercase\n",
    "        s = s.lower()\n",
    "        s = [words if words not in stop_words else '' for words in s.split(' ')]\n",
    "        doc = []\n",
    "        doc = list(filter(None, s)) \n",
    "        stemmed = []\n",
    "        \n",
    "        #stemming\n",
    "        for i in doc:\n",
    "            stemmed.append(ps.stem(i))\n",
    "            \n",
    "        #creating posting list\n",
    "        for x in stemmed:\n",
    "            key = x\n",
    "            dictionary.setdefault(key, [])\n",
    "            dictionary[key].append(doc_no)\n",
    "            \n",
    "        #removing duplicates\n",
    "        dictionary = {a:list(set(b)) for a, b in dictionary.items()}\n",
    "        \n",
    "    return dictionary,documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates positional index\n",
    "def positional_index(stop_words):\n",
    "    \n",
    "    dictionary = {}\n",
    "    documents = {}\n",
    "    for i in range(0,56):\n",
    "        doc_no = i\n",
    "        with open (\"data/trump_speechs/speech_\" + str(doc_no) + \".txt\",'r') as file:\n",
    "            s=file.read().replace('\\n',' ')[1:]\n",
    "        \n",
    "        #cleaning documents\n",
    "        s = re.sub('  ', ' ', s)\n",
    "        s = re.sub(r\"won't\", \"will not\", s)\n",
    "        s = re.sub(r\"can\\'t\", \"can not\", s)\n",
    "        s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "        s = re.sub(r\"\\'re\", \" are\", s)\n",
    "        s = re.sub(r\"\\'s\", \" is\", s)\n",
    "        s = re.sub(r\"\\'d\", \" would\", s)\n",
    "        s = re.sub(r\"\\'ll\", \" will\", s)\n",
    "        s = re.sub(r\"\\'t\", \" not\", s)\n",
    "        s = re.sub(r\"\\'ve\", \" have\", s)\n",
    "        s = re.sub(r\"\\'m\", \" am\", s)\n",
    "        s=re.sub(r'[^\\w\\s]',' ', s)\n",
    "        \n",
    "        key = 'speech_' + str(doc_no)\n",
    "        documents.setdefault(key,[])\n",
    "        documents[key].append(s)\n",
    "        \n",
    "        s = s.lower()\n",
    "        s = s.split(' ')\n",
    "        doc = []\n",
    "        doc = list(filter(None, s)) \n",
    "        temp_dict = {}\n",
    "        stemmed = []\n",
    "        \n",
    "        #stemming\n",
    "        for i in doc:\n",
    "            stemmed.append(ps.stem(i))\n",
    "        \n",
    "        #creating positional index posting lists\n",
    "        a = 0\n",
    "        for x in stemmed:\n",
    "            key = x\n",
    "            temp_dict.setdefault(key, [])\n",
    "            temp_dict[key].append(a)\n",
    "            a += 1\n",
    "        for x in temp_dict:\n",
    "            if dictionary.get(x):\n",
    "                dictionary[x][doc_no] = temp_dict.get(x)\n",
    "            else:\n",
    "                key = x\n",
    "                dictionary.setdefault(key, [])\n",
    "                dictionary[key] = {}\n",
    "                dictionary[x][doc_no] = temp_dict.get(x)\n",
    "            \n",
    "    return dictionary,documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Oct/2023 08:39:00] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Oct/2023 08:39:01] \"GET /static/stylesheets/display_style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [08/Oct/2023 08:39:10] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Oct/2023 08:39:12] \"GET /static/stylesheets/display_style.css HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#Getting stopwords from the file\n",
    "stop_words = []\n",
    "with open (\"data/stopword_en.txt\",'r') as file:\n",
    "    s=file.read().replace('\\n',' ')\n",
    "stop_words = s.split()\n",
    "\n",
    "#Getting inverted_index and positional_index\n",
    "dictionary_inverted,docu = inverted_index(stop_words)\n",
    "dictionary_positional,docu = positional_index(stop_words)\n",
    "\n",
    "#Returning Relevant document retrieved\n",
    "def documents_ret(a):\n",
    "    documents = {}\n",
    "    if(a):\n",
    "        for i in a:\n",
    "            speech = \"speech_\" + str(i)\n",
    "            documents.setdefault(speech,[])\n",
    "            documents[speech].append(docu.get(speech))\n",
    "    else:\n",
    "        documents = {}\n",
    "    \n",
    "    return documents\n",
    "        \n",
    "\n",
    "#Default page display/home_page\n",
    "@app.route('/')\n",
    "def dictionary():\n",
    "    return render_template('home.html')\n",
    "\n",
    "#Funtion will invoke whenever a query is posted\n",
    "@app.route(\"/query\", methods=['POST'])\n",
    "def upload():\n",
    "    #query processing start time\n",
    "    start = time.time()\n",
    "    #getting query from the HTML form\n",
    "    query = request.form['query']\n",
    "    #Checking for boolean,proximity and phrase queries\n",
    "    if('/' in query):\n",
    "        result = positional_query(query,dictionary_positional)\n",
    "    elif('\"' not in query):\n",
    "        result = process_query(query,dictionary_inverted)\n",
    "    else:\n",
    "        result = phrase_query(query,dictionary_positional,dictionary_inverted)\n",
    "\n",
    "    documents = documents_ret(result)\n",
    "    print(result)\n",
    "    end = time.time()\n",
    "    #total time to process query\n",
    "    times = end - start\n",
    "    return render_template('dictionary.html',dictionary = documents, num_docs= len(documents), time = str(times) + \" \" + \"seconds\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
